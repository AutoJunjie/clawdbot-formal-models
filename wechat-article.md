# 当AI助手连接你的设备：OpenClaw如何用形式化方法守护安全？

## 引言

想象一下这样的场景：你有一个AI助手，它不仅能和你聊天，还能执行Shell命令、浏览网页、控制你的iPhone和Mac、甚至帮你发消息。这就是OpenClaw——一个最近在技术圈引发热议的开源个人AI助手项目。它最初叫Clawdbot，因为商标问题改名为Moltbot，最终定名OpenClaw。

但问题来了：一个能做这么多事的AI助手，安全吗？

Palo Alto Networks的安全专家警告说，这类AI代理存在"致命三元组"（Lethal Trifecta）风险——它们能访问私密数据，会接触不可信内容，还具备执行外部通信的能力。当这三种能力汇聚在一个系统中时，任何一个环节的漏洞都可能导致灾难性的后果。

今天，我们来聊聊OpenClaw团队是如何用形式化方法来解决这个安全难题的。

---

## 一、AI助手的安全挑战

### 提示注入：AI时代的SQL注入

还记得SQL注入吗？在AI时代，我们有了新的注入方式——提示注入（Prompt Injection）。攻击者可以精心构造输入，让语言模型忽略原有指令，执行恶意操作。

更可怕的是间接注入。攻击指令可能藏在你收到的邮件里，隐匿在你浏览的网页中，甚至嵌入你下载的文档内。当AI助手帮你处理这些内容时，它可能在不知不觉中执行了攻击者的命令。

### 多通道攻击面

OpenClaw可以同时连接WhatsApp、Telegram、Slack、Discord等多个平台。这意味着攻击者只需要突破其中一个渠道，就能访问整个系统。五个平台就是五扇门，每扇门都有不同的锁，每把锁都可能存在弱点。

### 会话混淆

如果系统错误地把不同用户的对话混在一起，A用户的私密信息可能泄露给B用户。在群聊场景下，这个问题更加复杂——攻击者可能通过精心设计的交互，窃取受害者的私人对话历史。

### 权限提升

工具授权通常涉及多层策略：全局默认、代理级覆盖、会话级限制。这些层次相互叠加，形成复杂的权限矩阵。策略评估中的任何bug都可能导致权限被意外放大，让本不该执行的命令得以运行。

---

## 二、为什么传统测试不够用？

面对这些安全挑战，传统的单元测试和集成测试显得力不从心。

首先是并发场景的问题。当多个用户同时发起请求时，系统需要正确处理所有可能的交错顺序。两个请求可能有几十种交错方式，三个请求可能有几百种，而真实系统可能同时处理成百上千个请求。

其次是状态空间爆炸。假设有5种绑定模式、4种认证模式、3种会话范围、10种工具策略——组合起来就是600多种配置。每种配置还要考虑多用户并发访问的各种交错，测试用例的数量呈指数级增长。

还有边界条件的问题。很多安全bug只在特定条件组合下出现，比如"当认证模式为auto且绑定模式为lan且同时有两个配对请求时"。这种条件组合很难被随机测试覆盖。

最后是回归检测的挑战。代码修改可能在无意中破坏原有的安全属性，而这种破坏往往要等到生产环境出现问题才被发现。

面对组合爆炸的状态空间和微妙的安全属性，我们需要更强大的工具。这时候，形式化方法就派上用场了。

---

## 三、TLA+：让数学来证明安全

### 什么是TLA+？

TLA+（Temporal Logic of Actions）是一种形式化规约语言，由图灵奖得主Leslie Lamport发明。它用数学方式描述系统行为：定义系统启动时的初始状态，描述系统如何从一个状态转移到另一个状态，并声明在所有可达状态下必须成立的不变量。

TLC是TLA+的模型检查器。它会穷举所有可能的状态，检查是否存在违反不变量的情况。如果找到违反，它会输出一条从初始状态到违规状态的完整路径——这就是所谓的"反例"，直接告诉你系统是怎么出问题的。

### 为什么选择TLA+？

亚马逊AWS早就在用TLA+验证DynamoDB、S3、EBS等核心服务的正确性。微软用它验证Azure Cosmos DB。这些工业实践证明，TLA+不是学术界的玩具，而是能解决真实问题的工具。

OpenClaw团队把这种方法引入AI安全领域。正如他们在论文中所说："我们不是用TLA+做完整的程序验证，而是把它当作安全回归测试套件——可执行的规约，记录安全属性，检测回归。"

这个定位很务实。完整的程序验证对TypeScript代码库来说不切实际，但用TLA+验证关键的安全属性是完全可行的。

---

## 四、OpenClaw的安全属性体系

研究者为OpenClaw定义了六大类共91个安全属性。下面我们逐一介绍。

### 网关认证（P1）

网关认证是第一道防线。研究者定义了三个核心不变量：如果网关绑定到非本地地址且未启用认证，攻击者就能连接并执行命令；启用认证后，没有有效凭据的连接必须被拒绝；当系统运行在反向代理后面时，只能信任来自配置的trustedProxies的X-Forwarded-For头，防止IP欺骗。

这些不变量看起来简单，但在实际代码中很容易出错。比如"auto"模式下，系统需要根据是否配置了密码或令牌来决定认证方式，这个逻辑稍有不慎就会留下漏洞。

### DM配对和白名单（P2）

私信功能需要配对协议来控制谁能联系你。研究者为此定义了四个不变量：配对请求必须在TTL（默认1小时）后过期，防止旧请求被恶意利用；每个频道最多允许k个待处理的配对请求，防止攻击者通过请求洪水发起拒绝服务攻击；并发的配对批准操作不能破坏白名单存储的一致性；对同一发送者的重复批准不能创建重复的白名单条目。

### 会话隔离（P3）

会话隔离确保不同用户的对话不会混在一起。核心不变量是：没有显式身份链接的两个用户不能共享会话密钥，从而防止上下文泄露。

身份链接是一个有趣的概念——有时候你确实希望把不同平台上的同一个人关联起来。研究者为此定义了传递性不变量：如果A链接到B，B链接到C，那么A也应该链接到C。这个看似简单的属性，在实际实现中经常出错。

### 工具授权（P4）

工具授权是最复杂的部分。核心是"Deny Wins"原则：一旦某层策略拒绝了某个工具，后续层不能重新启用它。策略评估必须是单调递减的。

另一个关键不变量涉及提权执行：访问root/admin级别的工具需要全局配置和代理级配置同时允许，必须是AND关系而非OR关系。如果错误地使用了OR，攻击者只需要在任意一层获得权限就能提权。

工具组扩展也需要验证：当配置写"group:memory"时，它必须精确地扩展为memory_search、memory_get、memory_put这几个工具，不能多也不能少。

### 入口控制（P5）

入口控制处理消息如何进入系统。在需要@提及的群聊中，未授权发送者不能通过斜杠命令或特殊消息格式绕过提及要求。Webhook重试不能导致重复消息处理，每个事件ID只能被处理一次。

### 远程执行审批（P6）

远程执行是最危险的功能，需要严格的审批流程。研究者定义了三个不变量：需要审批的命令在pending状态下绝对不能执行；审批必须绑定到特定的请求ID，防止重放攻击；只有白名单内的命令才能执行。

重放攻击的防御尤其重要。如果审批只检查"是否已批准"而不检查"批准的是哪个请求"，攻击者就可以先让一个无害请求获得批准，然后用这个批准来执行恶意请求。

---

## 五、Green/Red测试范式

形式化验证有个常见陷阱：规约可能是空洞的。如果状态空间太小，或者初始条件太严格，模型检查器可能根本无法到达有意义的状态，于是"验证通过"实际上什么也没证明。

OpenClaw团队提出了Green/Red测试范式来解决这个问题。

Green模型是正确的实现，预期通过验证。Red模型是故意引入bug的变体，预期产生反例。如果Red模型没有产生反例，说明规约太弱，需要加强。

举个具体例子。对于"Deny Wins"属性，Green版本的策略应用函数是：先取当前允许集合与本层Allow的交集，再移除本层的Deny项。这保证了一旦被拒绝，就无法重新启用。

Red版本故意把顺序搞错：先移除Deny项，再取并集。这意味着被第2层拒绝的工具，可能被第4层的Allow重新允许。

TLC在Red模型上立即找到了反例。反例显示memory_get工具虽然在Deny2中被拒绝，但因为它也在Allow4中，错误的实现让它重新出现在最终允许列表中。这个反例不仅证明了规约不是空洞的，还清晰地展示了bug会如何被利用。

---

## 六、实战发现的真实Bug

在规约开发过程中，研究者发现了三个潜在bug，这些bug在传统测试中很难被发现。

### 配对竞态条件

原始实现先检查待处理请求的数量是否超过上限，然后再添加新请求。这是典型的"检查时刻/使用时刻"（TOCTOU）漏洞。

TLC找到的反例非常清晰：假设MaxPending设为1。状态1时队列为空，两个请求都处于idle状态。状态2时请求s1调用BeginRequest，检查通过因为队列长度0小于上限1。状态3时请求s2也调用BeginRequest，同样检查通过因为队列还没变。状态4时s1提交，队列变成包含s1。状态5时s2也提交，队列变成包含s1和s2——违规发生，队列长度2超过了上限1。

修复方案是使用文件锁实现原子的检查-并-追加操作，确保检查和提交之间不会被其他请求插入。

### 身份链接不对称

身份链接数据结构允许不对称条目，即A链接到B但B没有链接到A。这违反了预期的等价关系语义，导致会话密钥的规范化在某些情况下产生不一致的结果。

### 工具组漂移

一致性提取工具发现，TypeScript代码中的group:memory已经扩展为包含memory_put，但形式化模型还是旧的定义，只包含memory_search和memory_get。如果不修复这个漂移，形式化验证的保证就会与实际行为脱节。

---

## 七、CI集成：让形式化验证成为日常

OpenClaw把TLA+验证集成到了GitHub Actions。每次push和pull request都会触发验证流程。

工作流包含两类任务。对于Green模型，运行TLC并期望成功退出。对于Red模型，运行TLC并期望以退出码12退出（表示找到违规）——如果Red模型"意外"通过了验证，CI会失败，提醒开发者规约可能出了问题。

整个验证流程在8分钟内完成。网关暴露模型探索约200个状态，耗时30秒。工具策略优先级模型探索约500个状态，耗时15秒。配对存储模型探索约35000个状态，耗时90秒。节点执行管道是最复杂的模型，探索约80000个状态，也只需要90秒。

在8个月的开发周期中，形式化验证在2次回归到达生产环境之前就捕获了它们。两次都是关于策略评估顺序的微妙变化，这种bug用传统测试很难发现，但对TLC来说只是状态空间中的另一条路径。

---

## 八、一致性提取：保持规约与代码同步

形式化规约面临的一个根本挑战是与实现脱节。TLA+规约是抽象模型，不是实际的TypeScript代码。如果两者漂移，验证的保证就失去了意义。

OpenClaw团队通过一致性提取来应对这个挑战。他们开发了脚本，从TypeScript源代码中提取实现常量（如工具组定义、策略层次），然后生成对应的TLA+配置文件。

例如，TypeScript代码中定义了TOOL_GROUPS对象，包含group:memory映射到memory_search、memory_get、memory_put的数组。提取脚本解析这个定义，生成TLA+常量GroupMemory等于这三个工具的集合。TLA+规约中的不变量则检查ExpandGroup("group:memory")的结果是否等于GroupMemory。

如果开发者在TypeScript中给工具组添加了新工具但忘了更新形式化模型，CI会失败。这种机制确保规约与实现保持同步。

---

## 九、局限性与未来

### 有界验证的局限

TLC执行的是有界模型检查，只能探索有限的状态空间。如果bug只在1000个并发请求时才会出现，而模型限制MaxPending为5，那这个bug就会被漏掉。

不过，大多数安全bug特别是授权逻辑错误涉及的是小规模交互模式。两三个并发请求就足以触发竞态条件，两三层策略就足以暴露优先级bug。TLC在这些场景下表现出色。

### 模型与实现的差距

TLA+规约是抽象，不是代码。在模型和实现之间转换时可能引入bug。OpenClaw通过一致性提取、命名对齐（如resolveGatewayAuth()对应ResolveMode）、代码注释链接等方式来缩小这个差距，但无法完全消除。

### 时序属性的覆盖

大部分规约验证的是安全属性，即"坏事不会发生"。活性属性，即"好事最终会发生"，更难规约和验证。目前只有少数几个模型涉及活性，如RetryTerminationHarness验证重试最终会终止。

### 未来方向

研究者提出了几个有前景的方向。自动规约生成是其中之一——用LLM从自然语言安全声明生成TLA+不变量，然后人工审核。运行时验证是另一个方向——将TLA+规约编译为运行时监控器，在生产环境中检测违规。密码协议集成也很重要——将授权模型与ProVerif或Tamarin的密码协议模型链接起来，提供从密码原语到访问控制决策的端到端保证。

---

## 十、对我们的启示

OpenClaw的实践给我们带来了几个重要启示。

第一，AI基础设施也需要形式化方法。当AI助手能执行shell命令、访问你的设备时，传统测试已经不够了。授权逻辑的正确性可能决定你的数据安全。形式化方法提供了一种系统性的方式来验证这些关键属性。

第二，Green/Red范式值得推广。在任何形式化验证项目中，都应该问自己：我的规约会不会是空洞的？Red模型是对抗这个问题的有效武器。它不仅能验证规约的有效性，还能产生有价值的反例作为文档。

第三，CI集成改变文化。当模型检查在每个PR上运行时，开发者自然会考虑修改如何影响安全属性。规约不再是落灰的文档，而是活的、可执行的安全测试。

第四，假设prompt injection会发生。OpenClaw的设计哲学是假设语言模型会被操纵，然后设计硬边界来限制爆炸半径。这比依赖prompt防御更可靠。形式化方法验证的正是这些硬边界的正确性。

---

## 结语

AI助手正在变得越来越强大，从简单的聊天机器人进化为能控制我们设备的自主代理。这种能力带来的安全挑战，需要超越传统测试的方法。

OpenClaw团队展示了一条可行的路径：用形式化方法验证AI基础设施的安全属性。91个TLA+模块、Green/Red测试范式、CI集成——这套方法论完全可以迁移到其他AI代理系统。

当你下次部署一个能执行代码的AI助手时，不妨问问自己：我怎么证明它是安全的？

形式化方法也许不是唯一的答案，但它肯定是答案的一部分。

---

**参考资料**

本文基于OpenClaw团队发表的学术论文"Model Checking Security Properties of AI Assistant Gateways: A TLA+ Case Study of OpenClaw"整理。完整的TLA+规约和CI配置可在GitHub仓库 github.com/openclaw/clawdbot-formal-models 获取。OpenClaw网关本身的代码在 github.com/openclaw/openclaw。
